{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataiku Coding Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import dataiku\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import dataiku.core.pandasutils as pdu\n",
    "from dataiku.doctor.preprocessing import PCA\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 3000)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_steps = []\n",
    "preparation_output_schema = {u'userModified': False, u'columns': [{u'type': u'bigint', u'name': u'Age'}, {u'type': u'string', u'name': u'Sex'}, {u'type': u'string', u'name': u'ChestPainType'}, {u'type': u'bigint', u'name': u'RestingBP'}, {u'type': u'bigint', u'name': u'Cholesterol'}, {u'type': u'bigint', u'name': u'FastingBS'}, {u'type': u'string', u'name': u'RestingECG'}, {u'type': u'bigint', u'name': u'MaxHR'}, {u'type': u'boolean', u'name': u'ExerciseAngina'}, {u'type': u'double', u'name': u'Oldpeak'}, {u'type': u'string', u'name': u'ST_Slope'}, {u'type': u'bigint', u'name': u'HeartDisease'}]}\n",
    "\n",
    "ml_dataset_handle = dataiku.Dataset('clean_dataset')\n",
    "ml_dataset_handle.set_preparation_steps(preparation_steps, preparation_output_schema)\n",
    "%time ml_dataset = ml_dataset_handle.get_dataframe(limit = 100000)\n",
    "\n",
    "print ('Base data has %i rows and %i columns' % (ml_dataset.shape[0], ml_dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection\n",
    "ml_dataset = ml_dataset[[u'HeartDisease', u'ST_Slope', u'RestingBP', u'FastingBS', u'ChestPainType', u'Sex', u'Oldpeak', u'MaxHR', u'ExerciseAngina', u'Cholesterol', u'Age', u'RestingECG']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# astype('unicode') does not work as expected\n",
    "\n",
    "def coerce_to_unicode(x):\n",
    "    if sys.version_info < (3, 0):\n",
    "        if isinstance(x, str):\n",
    "            return unicode(x,'utf-8')\n",
    "        else:\n",
    "            return unicode(x)\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "\n",
    "categorical_features = [u'ST_Slope', u'ChestPainType', u'Sex', u'ExerciseAngina', u'RestingECG']\n",
    "numerical_features = [u'RestingBP', u'FastingBS', u'Oldpeak', u'MaxHR', u'Cholesterol', u'Age']\n",
    "text_features = []\n",
    "from dataiku.doctor.utils import datetime_to_epoch\n",
    "for feature in categorical_features:\n",
    "    ml_dataset[feature] = ml_dataset[feature].apply(coerce_to_unicode)\n",
    "for feature in text_features:\n",
    "    ml_dataset[feature] = ml_dataset[feature].apply(coerce_to_unicode)\n",
    "for feature in numerical_features:\n",
    "    if ml_dataset[feature].dtype == np.dtype('M8[ns]') or (hasattr(ml_dataset[feature].dtype, 'base') and ml_dataset[feature].dtype.base == np.dtype('M8[ns]')):\n",
    "        ml_dataset[feature] = datetime_to_epoch(ml_dataset[feature])\n",
    "    else:\n",
    "        ml_dataset[feature] = ml_dataset[feature].astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {u'1': 1, u'0': 0}\n",
    "ml_dataset['__target__'] = ml_dataset['HeartDisease'].map(str).map(target_map)\n",
    "del ml_dataset['HeartDisease']\n",
    "\n",
    "\n",
    "# Remove rows for which the target is unknown.\n",
    "ml_dataset = ml_dataset[~ml_dataset['__target__'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = pdu.split_train_valid(ml_dataset, prop=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows_when_missing = []\n",
    "impute_when_missing = [{'impute_with': u'MEAN', 'feature': u'RestingBP'}, {'impute_with': u'MEAN', 'feature': u'FastingBS'}, {'impute_with': u'MEAN', 'feature': u'Oldpeak'}, {'impute_with': u'MEAN', 'feature': u'MaxHR'}, {'impute_with': u'MEAN', 'feature': u'Cholesterol'}, {'impute_with': u'MEAN', 'feature': u'Age'}]\n",
    "\n",
    "# Features for which we drop rows with missing values\"\n",
    "for feature in drop_rows_when_missing:\n",
    "    train = train[train[feature].notnull()]\n",
    "    test = test[test[feature].notnull()]\n",
    "    print ('Dropped missing records in %s' % feature)\n",
    "\n",
    "# Features for which we impute missing values\"\n",
    "for feature in impute_when_missing:\n",
    "    if feature['impute_with'] == 'MEAN':\n",
    "        v = train[feature['feature']].mean()\n",
    "    elif feature['impute_with'] == 'MEDIAN':\n",
    "        v = train[feature['feature']].median()\n",
    "    elif feature['impute_with'] == 'CREATE_CATEGORY':\n",
    "        v = 'NULL_CATEGORY'\n",
    "    elif feature['impute_with'] == 'MODE':\n",
    "        v = train[feature['feature']].value_counts().index[0]\n",
    "    elif feature['impute_with'] == 'CONSTANT':\n",
    "        v = feature['value']\n",
    "    train[feature['feature']] = train[feature['feature']].fillna(v)\n",
    "    test[feature['feature']] = test[feature['feature']].fillna(v)\n",
    "    print ('Imputed missing values in feature %s with value %s' % (feature['feature'], coerce_to_unicode(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_DUMMIES = 100\n",
    "\n",
    "categorical_to_dummy_encode = [u'ST_Slope', u'ChestPainType', u'Sex', u'ExerciseAngina', u'RestingECG']\n",
    "\n",
    "# Only keep the top 100 values\n",
    "def select_dummy_values(train, features):\n",
    "    dummy_values = {}\n",
    "    for feature in categorical_to_dummy_encode:\n",
    "        values = [\n",
    "            value\n",
    "            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n",
    "        ]\n",
    "        dummy_values[feature] = values\n",
    "    return dummy_values\n",
    "\n",
    "DUMMY_VALUES = select_dummy_values(train, categorical_to_dummy_encode)\n",
    "\n",
    "def dummy_encode_dataframe(df):\n",
    "    for (feature, dummy_values) in DUMMY_VALUES.items():\n",
    "        for dummy_value in dummy_values:\n",
    "            dummy_name = u'%s_value_%s' % (feature, coerce_to_unicode(dummy_value))\n",
    "            df[dummy_name] = (df[feature] == dummy_value).astype(float)\n",
    "        del df[feature]\n",
    "        print ('Dummy-encoded feature %s' % feature)\n",
    "\n",
    "dummy_encode_dataframe(train)\n",
    "\n",
    "dummy_encode_dataframe(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_features = {u'RestingBP': u'AVGSTD', u'Age': u'AVGSTD', u'Oldpeak': u'AVGSTD', u'MaxHR': u'AVGSTD', u'Cholesterol': u'AVGSTD', u'FastingBS': u'AVGSTD'}\n",
    "for (feature_name, rescale_method) in rescale_features.items():\n",
    "    if rescale_method == 'MINMAX':\n",
    "        _min = train[feature_name].min()\n",
    "        _max = train[feature_name].max()\n",
    "        scale = _max - _min\n",
    "        shift = _min\n",
    "    else:\n",
    "        shift = train[feature_name].mean()\n",
    "        scale = train[feature_name].std()\n",
    "    if scale == 0.:\n",
    "        del train[feature_name]\n",
    "        del test[feature_name]\n",
    "        print ('Feature %s was dropped because it has no variance' % feature_name)\n",
    "    else:\n",
    "        print ('Rescaled %s' % feature_name)\n",
    "        train[feature_name] = (train[feature_name] - shift).astype(np.float64) / scale\n",
    "        test[feature_name] = (test[feature_name] - shift).astype(np.float64) / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.drop('__target__', axis=1)\n",
    "test_X = test.drop('__target__', axis=1)\n",
    "\n",
    "train_Y = np.array(train['__target__'])\n",
    "test_Y = np.array(test['__target__'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### logistic regression model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty=\"l1\",random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(train_X, train_Y)\n",
    "%time _predictions = clf.predict(test_X)\n",
    "%time _probas = clf.predict_proba(test_X)\n",
    "predictions = pd.Series(data=_predictions, index=test_X.index, name='predicted_value')\n",
    "cols = [\n",
    "    u'probability_of_value_%s' % label\n",
    "    for (_, label) in sorted([(int(target_map[label]), label) for label in target_map])\n",
    "]\n",
    "probabilities = pd.DataFrame(data=_probas, index=test_X.index, columns=cols)\n",
    "\n",
    "# Build scored dataset\n",
    "results_test = test_X.join(predictions, how='left')\n",
    "results_test = results_test.join(probabilities, how='left')\n",
    "results_test = results_test.join(test['__target__'], how='left')\n",
    "results_test = results_test.rename(columns= {'__target__': 'HeartDisease'})\n",
    "#performance\n",
    "from dataiku.doctor.utils.metrics import mroc_auc_score\n",
    "test_Y_ser = pd.Series(test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### logistic regression model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset2 and run the previous steps \n",
    "preparation_steps = []\n",
    "preparation_output_schema = {u'userModified': False, u'columns': [{u'type': u'bigint', u'name': u'Age'}, {u'type': u'string', u'name': u'Sex'}, {u'type': u'string', u'name': u'ChestPainType'}, {u'type': u'bigint', u'name': u'RestingBP'}, {u'type': u'bigint', u'name': u'Cholesterol'}, {u'type': u'bigint', u'name': u'FastingBS'}, {u'type': u'string', u'name': u'RestingECG'}, {u'type': u'bigint', u'name': u'MaxHR'}, {u'type': u'boolean', u'name': u'ExerciseAngina'}, {u'type': u'double', u'name': u'Oldpeak'}, {u'type': u'bigint', u'name': u'HeartDisease'}]}\n",
    "\n",
    "ml_dataset_handle = dataiku.Dataset('clean_dataset_2')\n",
    "ml_dataset_handle.set_preparation_steps(preparation_steps, preparation_output_schema)\n",
    "%time ml_dataset = ml_dataset_handle.get_dataframe(limit = 100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty=\"l1\",random_state=1337)\n",
    "%time clf.fit(train_X, train_Y)\n",
    "%time _predictions = clf.predict(test_X)\n",
    "%time _probas = clf.predict_proba(test_X)\n",
    "predictions = pd.Series(data=_predictions, index=test_X.index, name='predicted_value')\n",
    "cols = [\n",
    "    u'probability_of_value_%s' % label\n",
    "    for (_, label) in sorted([(int(target_map[label]), label) for label in target_map])\n",
    "]\n",
    "probabilities = pd.DataFrame(data=_probas, index=test_X.index, columns=cols)\n",
    "\n",
    "# Build scored dataset\n",
    "results_test = test_X.join(predictions, how='left')\n",
    "results_test = results_test.join(probabilities, how='left')\n",
    "results_test = results_test.join(test['__target__'], how='left')\n",
    "results_test = results_test.rename(columns= {'__target__': 'HeartDisease'})\n",
    "#performance\n",
    "from dataiku.doctor.utils.metrics import mroc_auc_score\n",
    "test_Y_ser = pd.Series(test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### random forest model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.drop('__target__', axis=1)\n",
    "test_X = test.drop('__target__', axis=1)\n",
    "\n",
    "train_Y = np.array(train['__target__'])\n",
    "test_Y = np.array(test['__target__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=155,\n",
    "    random_state=1337,\n",
    "    max_depth=13,\n",
    "    min_samples_leaf=4,\n",
    "    verbose=2)\n",
    "%time clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _predictions = clf.predict(test_X)\n",
    "%time _probas = clf.predict_proba(test_X)\n",
    "predictions = pd.Series(data=_predictions, index=test_X.index, name='predicted_value')\n",
    "cols = [\n",
    "    u'probability_of_value_%s' % label\n",
    "    for (_, label) in sorted([(int(target_map[label]), label) for label in target_map])\n",
    "]\n",
    "probabilities = pd.DataFrame(data=_probas, index=test_X.index, columns=cols)\n",
    "\n",
    "# Build scored dataset\n",
    "results_test = test_X.join(predictions, how='left')\n",
    "results_test = results_test.join(probabilities, how='left')\n",
    "results_test = results_test.join(test['__target__'], how='left')\n",
    "results_test = results_test.rename(columns= {'__target__': 'HeartDisease'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_data = []\n",
    "features = train_X.columns\n",
    "for feature_name, feature_importance in zip(features, clf.feature_importances_):\n",
    "    feature_importances_data.append({\n",
    "        'feature': feature_name,\n",
    "        'importance': feature_importance\n",
    "    })\n",
    "\n",
    "# Plot the results\n",
    "pd.DataFrame(feature_importances_data)\\\n",
    "    .set_index('feature')\\\n",
    "    .sort_values(by='importance')[-10::]\\\n",
    "    .plot(title='Top 10 most important variables',\n",
    "          kind='barh',\n",
    "          figsize=(10, 6),\n",
    "          color='#348ABD',\n",
    "          alpha=0.6,\n",
    "          lw='1',\n",
    "          edgecolor='#348ABD',\n",
    "          grid=False,)\n",
    "#performance\n",
    "from dataiku.doctor.utils.metrics import mroc_auc_score\n",
    "test_Y_ser = pd.Series(test_Y)\n",
    "print ('AUC value:', mroc_auc_score(test_Y_ser, _probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### random forest model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.drop('__target__', axis=1)\n",
    "test_X = test.drop('__target__', axis=1)\n",
    "\n",
    "train_Y = np.array(train['__target__'])\n",
    "test_Y = np.array(test['__target__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=131,\n",
    "    random_state=1337,\n",
    "    max_depth=15,\n",
    "    min_samples_leaf=1,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _predictions = clf.predict(test_X)\n",
    "%time _probas = clf.predict_proba(test_X)\n",
    "predictions = pd.Series(data=_predictions, index=test_X.index, name='predicted_value')\n",
    "cols = [\n",
    "    u'probability_of_value_%s' % label\n",
    "    for (_, label) in sorted([(int(target_map[label]), label) for label in target_map])\n",
    "]\n",
    "probabilities = pd.DataFrame(data=_probas, index=test_X.index, columns=cols)\n",
    "\n",
    "# Build scored dataset\n",
    "results_test = test_X.join(predictions, how='left')\n",
    "results_test = results_test.join(probabilities, how='left')\n",
    "results_test = results_test.join(test['__target__'], how='left')\n",
    "results_test = results_test.rename(columns= {'__target__': 'HeartDisease'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_data = []\n",
    "features = train_X.columns\n",
    "for feature_name, feature_importance in zip(features, clf.feature_importances_):\n",
    "    feature_importances_data.append({\n",
    "        'feature': feature_name,\n",
    "        'importance': feature_importance\n",
    "    })\n",
    "\n",
    "# Plot the results\n",
    "pd.DataFrame(feature_importances_data)\\\n",
    "    .set_index('feature')\\\n",
    "    .sort_values(by='importance')[-10::]\\\n",
    "    .plot(title='Top 10 most important variables',\n",
    "          kind='barh',\n",
    "          figsize=(10, 6),\n",
    "          color='#348ABD',\n",
    "          alpha=0.6,\n",
    "          lw='1',\n",
    "          edgecolor='#348ABD',\n",
    "          grid=False,)\n",
    "#performance\n",
    "from dataiku.doctor.utils.metrics import mroc_auc_score\n",
    "test_Y_ser = pd.Series(test_Y)\n",
    "print ('AUC value:', mroc_auc_score(test_Y_ser, _probas))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
